{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now continuing to last code of Layers we did,\n",
    "# Here I will create a Class for the same, for better code management "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard ML codes name inputs as X\n",
    "\n",
    "X = [[11.8,-2.6,3.2, -5.0],\n",
    "       [1.2,4.8,5.6,-6.1],\n",
    "       [2.0, 5.0, -1.5, 0.6]]  ## assuming this is the input data to the neural network\n",
    "\n",
    "# Now we are going to define Two Hidden Layers\n",
    "\n",
    "# we call it hidden, as we programmers are not incharge of how that layer changes on our own,\n",
    "## but the neural network, optimizers and other params are responsible to operate the hidden layers\n",
    "\n",
    "class layer_dense:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,):\n",
    "        pass\n",
    "# these are basic methods that we will use in our class    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Class description:\n",
    "\n",
    "# Initialzing weights . . . \n",
    "### The weights are initialized randomly, btw range (-1,1) generally but more tighter/smaller the range\n",
    "\n",
    "#### The reason we want small values btw -1,1 so we can tune at micro level, bigger values i.e above 1, makes bigger n bigger tunings, and ultimately explosion in values !!!\n",
    "\n",
    "### Now, our Input data might have values that exclude this range, so we use normalziation methods for the same to scale them accordingly\n",
    "\n",
    "##### This is the one of core reason of feature scaling too, this we will do in later codes\n",
    "\n",
    "### When we initialize our layers, we will adhere to principal of keeping our values small, rather than exploding them \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Initializing bias . . . \n",
    "### In this case we can keep bias initially as zero, but in many cases we might see, the neural input, and weights are so small that ultimately the neuron wont fire much of the big output and having zero bias being added will result overall output as negligible/ zero -- Dead Neural Layer/ Network\n",
    "\n",
    "## So better bias to be initialized with Non zero number\n",
    "\n",
    "# Lets Do It !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "class layer_dense:\n",
    "    def __init__(self,n_inputs, n_neurons):\n",
    "        self.n_neurons = n_neurons\n",
    "        self.weights = 0.10*np.random.randn(n_inputs, n_neurons) \n",
    "        print(\"Created a hidden layer with \"+ str(self.n_neurons)+\" neurons\")\n",
    "# here we need the shape we wanna pass for the weights\n",
    "## i.e we need the size of input coming in, and size of neurons the user want to have\n",
    "## multiplying 0.10 so we have tighter weightage range\n",
    "# the actual shape of weights is , n_neurons* n_inputs\n",
    "# but notice here we are doing opposite,\n",
    "# the reason is we wont be needing Transpose method during further Forward Pass process\n",
    "\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "        \n",
    "\n",
    "    def forward(self,inputs):   # this method will just take the inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a hidden layer with 5 neurons\n"
     ]
    }
   ],
   "source": [
    "X = [[11.8,-2.6,3.2, -5.0],\n",
    "       [1.2,4.8,5.6,-6.1],\n",
    "       [2.0, 5.0, -1.5, 0.6]]  \n",
    "\n",
    "# object instantiazation - creating layer 1 \n",
    "layer1 = layer_dense(4,5)   # takes (input_size, desired_num_neurons)\n",
    "# input size is the size of each vector inside the batch , here X[0] == 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a hidden layer with 2 neurons\n"
     ]
    }
   ],
   "source": [
    "# output of layer 1 is the input of layer 2\n",
    "# n_neurons is 5 on layer 1, and the output size will be 5 for layer 1\n",
    "# hence input of layer 2 is 5\n",
    "\n",
    "layer2 = layer_dense(5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02568743  0.29834814 -2.29495758 -0.80543781 -0.38865618]\n",
      " [-0.22632314  0.60491415 -0.95917239  1.65329946 -0.34248843]\n",
      " [ 0.24016246  0.82861281  0.72622272 -0.97927288 -0.15705039]]\n"
     ]
    }
   ],
   "source": [
    "layer1.forward(X)\n",
    "\n",
    "print(layer1.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2.forward(X)\n",
    "\n",
    "print(layer1.output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
