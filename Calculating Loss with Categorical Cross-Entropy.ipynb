{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here I am going to implement Loss function and Categorical Cross-Entropy which we apply to output layer to the classifier function.\n",
    "\n",
    "\n",
    "##### After Feed Forwarding, next step comes is Back Propagation & Optimization but before that, we must find the metrics of error/ how wrong our model is.\n",
    "\n",
    "\n",
    "#### Some people will stick to the accuracy, so that's not right approach to follow. If we cared only about accuracy of the model, we would be feeding Binary True or either Binary false to the optimizer attempting to optimize the NN.\n",
    "\n",
    "#### But Neural Networks actually outputs a probability distribution, a confidence score for each of classes. This makes optimizer into a bad place in finding right model. \n",
    "#### So we use loss functions.\n",
    "### One simple example of a Loss function is *Mean Absolute Error* \n",
    "### In general Loss function for classification method, that uses softmax as activation function in output layer is *Categorical Crossentropy*\n",
    "\n",
    "```sh\n",
    "Formula for this is to take the negative sum of the target values, multiplied by the log of the predicted values for each of the values in the distribution\n",
    "```\n",
    "## One hot encoding :\n",
    "### Idea of one hot encoding is, if we have a vector of *n* classes long. The vector is filled with zero except the index of the target class, where we will have one.\n",
    "\n",
    "### If we have Classes : 3, then\n",
    "#### Label - 0\n",
    "##### One-hot - [1, 0 , 0]\n",
    "#### Label - 1\n",
    "##### One-hot - [0, 1, 0]\n",
    "#### Label - 2\n",
    "##### One-hot - [0, 0, 1]\n",
    "\n",
    "\n",
    "## Log : \n",
    "\n",
    "### Natural Logarithm : (with base e)\n",
    "#### y = log_e x = ln(x)\n",
    "#### its, euler's number, e = 2.71828182...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6486586255873816\n",
      "\n",
      "181.27224187515117\n"
     ]
    }
   ],
   "source": [
    "# Calculation of logarithm\n",
    "\n",
    "\"\"\"\n",
    "log solving for x\n",
    "\n",
    "is  e ** x = b\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "b = 5.2 \n",
    "\n",
    "print(np.log(b))\n",
    "## e ** x\n",
    "print()\n",
    "print(math.e ** b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating  Loss with Categorical Cross-Entropy\n",
    "\n",
    "### Classes : 3, then\n",
    "#### Label - 0\n",
    "##### One-hot - [1, 0 , 0]\n",
    "#### Prediction from NN :- [0.7, 0.1, 0.2]\n",
    "\n",
    "### L = -(1*log(0.7) + 0*log(0.1) + 0*log(0.2)) = 0.35667494...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "## Coding Categorical cross entropy on an example\n",
    "\n",
    "import math\n",
    "\n",
    "# creating dummy output here\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "\n",
    "# for target class 0\n",
    "target_class_onehot = [1, 0, 0]\n",
    "\n",
    "# using math.log instead of np.log as we are coding NN with math functions\n",
    "## not libraries\n",
    "\n",
    "loss = -(math.log(softmax_output[0]*target_class_onehot[0] + \n",
    "                 softmax_output[1]*target_class_onehot[1] +  # is 0\n",
    "                 softmax_output[2]*target_class_onehot[2]))  # is 0\n",
    "# or we can just do:- -(math.log(np.dot(softmax_output,target_class_onehot)))\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "## entire output then becomes same as\n",
    "## target_class_onehot[0]  =  1, target_class_onehot[1 & 2 ] = 0\n",
    "## so,\n",
    "loss = -math.log(softmax_output[0])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# understanding loss \n",
    "\n",
    "#### softmax_output[0]  is  0.7\n",
    "#### softmax_output[1] is 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245  Here the loss is lower, as our target class is Zero\n"
     ]
    }
   ],
   "source": [
    "print(-(math.log(0.7)),\" Here the loss is lower, as our target class is Zero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599453 Here the loss is higher,as our target class is Zero, not One\n"
     ]
    }
   ],
   "source": [
    "print(-(math.log(0.5)),\"Here the loss is higher,as our target class is Zero, not One\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6094379124341003 Here the loss is higher,as our target class is Zero, not 3\n"
     ]
    }
   ],
   "source": [
    "print(-(math.log(0.2)),\"Here the loss is higher,as our target class is Zero, not 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's how loss function through categorical cross-entropy works !!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
