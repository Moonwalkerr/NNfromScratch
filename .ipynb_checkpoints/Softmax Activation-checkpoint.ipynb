{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook explains the Softmax Activation Function in detail\n",
    "\n",
    "\n",
    "\n",
    "## Now first question after learning multiple diff activation functions like Sigmoid and ReLU, why another activation function ?\n",
    "\n",
    "\n",
    "### Imagine these are our output values from a layer :-\n",
    "```sh\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "```\n",
    "### Now what do we do with these ?\n",
    "\n",
    "### If we are only predicting, then we would consider , whichever is the largest value from the layer_outputs is our Prediction. Here layer_outputs[0] = 4.8, is our prediction\n",
    "\n",
    "\n",
    "## We also have to remeber we are trying to train our Neural Networks as well along with the predicting values.\n",
    "\n",
    "### We ll learn in upcoming notebooks on training NNs\n",
    "\n",
    "### But First Step in training the model is to determine *how wrong is our model ?* \n",
    "\n",
    "## Which one of these is more correct ?\n",
    "```sh\n",
    "layer_outputs = [4.8, 1.21, 2.385] \n",
    "(Most People will consider this one as this has relatively larger neuron output at index 0) \n",
    "layer_outputs = [4.8, 4.79, 4.25]\n",
    "```\n",
    "### Accuracy Wise, they are both identical (coz both have 4.8 as prediction)\n",
    "\n",
    "### First step to measure how wrong the output is to compare them relatively to other neurons. This can't be done using ReLU as ReLU doesn't provides per neuron basis comparisions of outputs\n",
    "\n",
    "### Next problem is these both are unbouned, so relatively closeness can vary btw both samples, and there r lots of issues with no solid way to determine how wrong our model is.\n",
    "\n",
    "## This is why we need some new activation function :- *Softmax Activation Function*\n",
    "\n",
    "### Goal :- Let's say we need to classify cats & dogs, we have 2 neurons at output layer. So our goal would be to get the outputs from the 2 neurons in the way of *probability distribution* that if the input is a cat, first neuron shows 0.80 - 0.90 or 1.0 (upto 100 % probability distribution)  and second neuron outputs 0.08, 0.12, etc like values (i.e low confidence score for it being a dog)\n",
    "\n",
    "\n",
    "\n",
    "### This will help us to determine how wrong / right our model is.\n",
    "\n",
    "\n",
    "### If we use a probability distribution, and keep ReLU as function:\n",
    "```sh\n",
    "ReLU(0) = 0,.0000 % \n",
    "ReLU(1) = 1, 100.0000 % \n",
    "```\n",
    "### The problem here is, if any of those ouput values is negative, then ReLU will clip it and yeild Zero.\n",
    "\n",
    "### ReLU(-1) / ReLU(-9000) / ReLU(-21) = 0, 0.000 %\n",
    "\n",
    "#### What if all values are negative ? This doesn't makes any sense\n",
    "#### There are some other methods but research shows they also aren't effective\n",
    "\n",
    "## Exponential Function \n",
    "```sh\n",
    "y = e^x (\"e ~ 2.718281828459045, Euler's Number\") \n",
    "if x = -10.0000\n",
    "y = e^-10.0000 ~ + 0.00005\n",
    "```\n",
    "\n",
    "### What this function actually does is just to make sure no value is actually negative, at the output after passing through it. So we can calculate how accurate the output is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121.51041751873483, 3.353484652549023, 10.859062664920513]\n"
     ]
    }
   ],
   "source": [
    "# Coding for exponentiation function\n",
    "\n",
    "import math\n",
    "\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "\n",
    "E = math.e\n",
    "\n",
    "exp_values = []\n",
    "\n",
    "for output in layer_outputs:\n",
    "    exp_values.append(E**output)\n",
    "print(exp_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After exponentiation, next step is to normalize these values\n",
    "\n",
    "#### In our case, the output of single neuron. is divided by the sum of the output neurons in the output layer\n",
    "\n",
    "#### This gives us the probability distribution that we want.\n",
    "\n",
    "#### This requires us to get rid of all the negative values without losing the information of those negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8952826639572619, 0.024708306782099374, 0.0800090292606387] , Sum of Norm Values :0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "## coding Normalization\n",
    "\n",
    "norm_base = sum(exp_values)\n",
    "norm_values = []\n",
    "\n",
    "for value in exp_values:\n",
    "    norm_values.append(value / norm_base)\n",
    "print(norm_values,\", Sum of Norm Values :\"+str(sum(norm_values)))\n",
    "\n",
    "### exp_values = exponentiated values\n",
    "### norm_values = normalized exponentiated values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quicker way to calculate both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp Values [121.51041752   3.35348465  10.85906266]\n",
      "Norm Exp Values [0.89528266 0.02470831 0.08000903]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs  # not necessary, just following nnfs code in video\n",
    "nnfs.init() # --\"--\n",
    "\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "exp_values = np.exp(layer_outputs)  # numpy's exp function \n",
    "norm_values = exp_values / np.sum(exp_values)\n",
    "print(\"Exp Values\",exp_values)\n",
    "print(\"Norm Exp Values\",norm_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we did here is :-\n",
    "## layer_output -->  Exponentiate --> Normalize --> Output \n",
    "\n",
    "### This can be written as :-\n",
    "## layer_output --> *Softmax* --> Output\n",
    "\n",
    "### Exponentiation and Normalization together are called The Softmax Function ! ! \n",
    "### Now we can easily understand what softmax activation function really is and why we are applying it.\n",
    "\n",
    "# Moving ahead,\n",
    "## In above code, we were dealing with *Single Vector of Output*, but in real, we will be dealing with a *Batch of Outputs*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.21510418e+02, 3.35348465e+00, 1.08590627e+01],\n",
       "       [7.33197354e+03, 7.42735782e-02, 3.72665317e-06],\n",
       "       [2.68317287e+00, 6.73794700e-03, 4.48168907e+00]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## dynamic code that can deal with batches \n",
    "\n",
    "layer_outpus = [[4.8, 1.21, 2.385],\n",
    "               [8.9, -2.6, -12.5],\n",
    "               [0.987, -5, 1.5]]\n",
    "\n",
    "\n",
    "exp_values = np.exp(layer_outpus) # by default np exps over individual values\n",
    "exp_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.31799999999999784"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(layer_outpus) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### but we want to have set of 3 sums from the 3 diff vectors inside the batch\n",
    "#### np doesnt do this by default , rather takes all individual values and sums up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.35722965e+02, 7.33204782e+03, 7.17159988e+00])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to take sum across rows\n",
    "np.sum(exp_values,axis = 1)   # axis = 0 is sum across rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But for finding Norm Values, we need to keep the dims of sum of the exponential values in same as the exp_values are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.35722965e+02],\n",
       "       [7.33204782e+03],\n",
       "       [7.17159988e+00]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This keepdims will keep the matrix of same dimension as exp_values\n",
    "\n",
    "np.sum(exp_values,axis = 1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.21510418e+02, 3.35348465e+00, 1.08590627e+01],\n",
       "       [7.33197354e+03, 7.42735782e-02, 3.72665317e-06],\n",
       "       [2.68317287e+00, 6.73794700e-03, 4.48168907e+00]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_values   # see same dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Normalized Values are : \n",
      " [[8.95282664e-01 2.47083068e-02 8.00090293e-02]\n",
      " [9.99989870e-01 1.01299910e-05 5.08269076e-10]\n",
      " [3.74138673e-01 9.39531919e-04 6.24921795e-01]]\n"
     ]
    }
   ],
   "source": [
    "# implementation\n",
    "\n",
    "\n",
    "layer_outpus = [[4.8, 1.21, 2.385],\n",
    "               [8.9, -2.6, -12.5],\n",
    "               [0.987, -5, 1.5]]\n",
    "\n",
    "\n",
    "exp_values = np.exp(layer_outpus) \n",
    "\n",
    "\n",
    "norm_values = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "print(\"The Normalized Values are : \\n\",norm_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One slight issue with exponentiation is explosion of values as the input into exp function grows.\n",
    "```sh\n",
    "np.exp(1) = 2.781281....\n",
    "\n",
    "np.exp(10) = 22026.465794....\n",
    "\n",
    "np.exp(100) = 2.68811714...e+43  # doesnt takes too much to get massive nos\n",
    "\n",
    "np.exp(1000) = given overflow error in python\n",
    "```\n",
    "\n",
    "## Overflow Prevention \n",
    "```sh\n",
    "v = u - max(u)  \n",
    "# use this in output layer prior to exponentiation\n",
    "# substract the largest value in the layer from all of the values in layer\n",
    "\n",
    "\n",
    "if output, u = [1, 2, 3],  max(u) = 3\n",
    "\n",
    "\n",
    " v = [1, 2, 3] - 3 = [-2, -1 , 0]\n",
    "\n",
    "So, the largest value, now prior to exponentiation is zero (after applying this operation), then range of possibilities becomes somewhere btw (0,1) after exponentiation\n",
    "\n",
    "```\n",
    "\n",
    "#### The actual output with or without applying overflow prevention method, is actually the same, so we are just preventing lots of time and space complexcity issues by doing the same\n",
    "\n",
    "## Now putting all above together in our Fundamental NN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs \n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "layer_outpus = [[4.8, 1.21, 2.385],\n",
    "               [8.9, -2.6, -12.5],\n",
    "               [0.987, -5, 1.5]]\n",
    "\n",
    "X, y = spiral_data(100,3)\n",
    "class layer_dense:\n",
    "    def __init__(self,n_inputs, n_neurons):\n",
    "        self.n_neurons = n_neurons\n",
    "        self.weights = 0.10*np.random.randn(n_inputs, n_neurons) \n",
    "        print(\"Created a hidden layer with \"+ str(self.n_neurons)+\" neurons\")\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "    \n",
    "    def forward(self,inputs):   # this method will just take the inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "class activation_relu:\n",
    "        def forward(self, inputs):\n",
    "            self.output= np.maximum(0,inputs)\n",
    "\n",
    "class activation_softmax:\n",
    "        def forward(self, inputs):\n",
    "            exp_values = np.exp(inputs - np.max(inputs,axis=1,keepdims=True))\n",
    "            probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True) \n",
    "            self.output = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a hidden layer with 3 neurons\n"
     ]
    }
   ],
   "source": [
    "X,y = spiral_data(100,3)   # has 2 input class\n",
    "\n",
    "\n",
    "dense1 = layer_dense(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a hidden layer with 3 neurons\n"
     ]
    }
   ],
   "source": [
    "activation1 = activation_relu()\n",
    "\n",
    "### output of the prv layer was a 3 (3 neurons) so input for this layer is 3\n",
    "dense2 = layer_dense(3,3)  \n",
    "\n",
    "# taking 3 as output neurons and treating this as output layer\n",
    "\n",
    "activation2 = activation_softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.3333338 , 0.33333224, 0.333334  ],\n",
       "       [0.33334652, 0.33330026, 0.33335325],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33336297, 0.3332589 , 0.3333781 ],\n",
       "       [0.33335266, 0.3332848 , 0.33336252],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33348224, 0.333216  , 0.33330175],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33348945, 0.33321032, 0.3333002 ],\n",
       "       [0.33351874, 0.33318728, 0.333294  ],\n",
       "       [0.33351728, 0.3331884 , 0.33329433],\n",
       "       [0.33366132, 0.33307493, 0.3332637 ],\n",
       "       [0.3337274 , 0.3330229 , 0.33324966],\n",
       "       [0.3336114 , 0.3331143 , 0.33327436],\n",
       "       [0.33366475, 0.33307222, 0.33326298],\n",
       "       [0.3340631 , 0.33275864, 0.33317828],\n",
       "       [0.3338192 , 0.33295062, 0.33323017],\n",
       "       [0.333816  , 0.33295316, 0.33323085],\n",
       "       [0.33416694, 0.33267692, 0.33315614],\n",
       "       [0.3342674 , 0.33259782, 0.3331347 ],\n",
       "       [0.33436042, 0.33248553, 0.33315408],\n",
       "       [0.33431408, 0.33256114, 0.3331248 ],\n",
       "       [0.3344566 , 0.33276573, 0.33277768],\n",
       "       [0.33449602, 0.33274332, 0.33276066],\n",
       "       [0.33444342, 0.3324594 , 0.3330972 ],\n",
       "       [0.33459476, 0.3325041 , 0.3329012 ],\n",
       "       [0.33459818, 0.33244604, 0.3329558 ],\n",
       "       [0.334272  , 0.33259422, 0.33313373],\n",
       "       [0.33467102, 0.33224633, 0.33308265],\n",
       "       [0.33471903, 0.3323734 , 0.33290756],\n",
       "       [0.33469188, 0.33210048, 0.33320764],\n",
       "       [0.33482566, 0.33247888, 0.33269545],\n",
       "       [0.33441582, 0.33158693, 0.33399728],\n",
       "       [0.3344306 , 0.33169234, 0.3338771 ],\n",
       "       [0.3348656 , 0.33200076, 0.3331337 ],\n",
       "       [0.33491138, 0.33198056, 0.3331081 ],\n",
       "       [0.33501127, 0.33209333, 0.33289543],\n",
       "       [0.33473036, 0.33173338, 0.3335363 ],\n",
       "       [0.33454856, 0.3316667 , 0.33378473],\n",
       "       [0.33456537, 0.3315768 , 0.3338578 ],\n",
       "       [0.33463284, 0.33119258, 0.33417457],\n",
       "       [0.33455795, 0.3310685 , 0.33437353],\n",
       "       [0.33459055, 0.33102024, 0.3343892 ],\n",
       "       [0.33471915, 0.33118215, 0.33409876],\n",
       "       [0.33468682, 0.33143175, 0.3338815 ],\n",
       "       [0.33391437, 0.33187306, 0.33421254],\n",
       "       [0.3347489 , 0.33083966, 0.3344115 ],\n",
       "       [0.33471632, 0.330785  , 0.33449873],\n",
       "       [0.33462456, 0.33078855, 0.3345869 ],\n",
       "       [0.3348884 , 0.33085728, 0.33425432],\n",
       "       [0.33445087, 0.3308807 , 0.33466846],\n",
       "       [0.3343508 , 0.3309629 , 0.33468634],\n",
       "       [0.33356023, 0.3327634 , 0.33367634],\n",
       "       [0.3333693 , 0.33324295, 0.33338773],\n",
       "       [0.3337151 , 0.3323742 , 0.33391073],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33338138, 0.33321267, 0.33340597],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.3336797 , 0.3330605 , 0.33325982],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33405206, 0.33276734, 0.33318064],\n",
       "       [0.33453482, 0.33238748, 0.33307767],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33394545, 0.33285123, 0.33320332],\n",
       "       [0.3355799 , 0.33156624, 0.33285385],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33554536, 0.33159336, 0.33286124],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33427867, 0.33258897, 0.33313233],\n",
       "       [0.3358892 , 0.33132344, 0.33278733],\n",
       "       [0.33394444, 0.33285204, 0.33320355],\n",
       "       [0.33523983, 0.33183336, 0.33292684],\n",
       "       [0.33486634, 0.33212683, 0.33300683],\n",
       "       [0.33676502, 0.33152914, 0.33170578],\n",
       "       [0.33678713, 0.33099684, 0.33221602],\n",
       "       [0.33687976, 0.3314561 , 0.33166412],\n",
       "       [0.336871  , 0.33095536, 0.3321736 ],\n",
       "       [0.33693263, 0.33097768, 0.33208966],\n",
       "       [0.33632323, 0.33098292, 0.3326938 ],\n",
       "       [0.33704627, 0.33132204, 0.3316317 ],\n",
       "       [0.33709043, 0.33105585, 0.33185372],\n",
       "       [0.3370039 , 0.33034497, 0.33265114],\n",
       "       [0.33670062, 0.32993746, 0.33336195],\n",
       "       [0.33613446, 0.32971644, 0.3341491 ],\n",
       "       [0.33665636, 0.3298053 , 0.3335383 ],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.3333714 , 0.33331642, 0.33331218],\n",
       "       [0.33339295, 0.33325964, 0.33334735],\n",
       "       [0.33343032, 0.3332247 , 0.3333449 ],\n",
       "       [0.3334866 , 0.33326334, 0.3332501 ],\n",
       "       [0.33346918, 0.33314797, 0.33338284],\n",
       "       [0.33350006, 0.3330681 , 0.33343184],\n",
       "       [0.3335203 , 0.33306786, 0.33341184],\n",
       "       [0.33354563, 0.33303475, 0.3334196 ],\n",
       "       [0.3336194 , 0.33300593, 0.3333747 ],\n",
       "       [0.3336941 , 0.33299366, 0.33331218],\n",
       "       [0.33360228, 0.3328178 , 0.33357996],\n",
       "       [0.33366644, 0.33279455, 0.33353895],\n",
       "       [0.33368868, 0.3327286 , 0.33358276],\n",
       "       [0.3334718 , 0.33298555, 0.33354264],\n",
       "       [0.3336111 , 0.33270428, 0.33368465],\n",
       "       [0.33364764, 0.33264393, 0.3337085 ],\n",
       "       [0.3336717 , 0.33259645, 0.33373183],\n",
       "       [0.33339214, 0.3331857 , 0.3334222 ],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33344135, 0.33306208, 0.3334966 ],\n",
       "       [0.33348098, 0.33296248, 0.3335565 ],\n",
       "       [0.33393678, 0.33231446, 0.33374873],\n",
       "       [0.3335089 , 0.33289242, 0.33359873],\n",
       "       [0.33353013, 0.332839  , 0.33363083],\n",
       "       [0.33341843, 0.33311963, 0.33346194],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33403996, 0.33205745, 0.33390263],\n",
       "       [0.33344704, 0.33304775, 0.3335052 ],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.333544  , 0.33280423, 0.33365178],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.3336007 , 0.3331227 , 0.33327663],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.3338775 , 0.33290473, 0.33321777],\n",
       "       [0.33352354, 0.33318347, 0.333293  ],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33387697, 0.33290514, 0.3332179 ],\n",
       "       [0.33423722, 0.33262163, 0.33314118],\n",
       "       [0.334758  , 0.33221197, 0.33303   ],\n",
       "       [0.3343448 , 0.33253697, 0.33311823],\n",
       "       [0.33481643, 0.33216608, 0.3330175 ],\n",
       "       [0.33404958, 0.33276922, 0.3331811 ],\n",
       "       [0.33436713, 0.33251938, 0.33311346],\n",
       "       [0.33493125, 0.33207577, 0.33299294],\n",
       "       [0.33450997, 0.33240706, 0.333083  ],\n",
       "       [0.3353511 , 0.33217108, 0.33247784],\n",
       "       [0.3353854 , 0.33207434, 0.33254027],\n",
       "       [0.33542618, 0.33205846, 0.33251536],\n",
       "       [0.33546507, 0.33216974, 0.33236516],\n",
       "       [0.3353229 , 0.33146968, 0.33320743],\n",
       "       [0.33552933, 0.3319197 , 0.33255097],\n",
       "       [0.33488664, 0.3321109 , 0.33300248],\n",
       "       [0.33557773, 0.33221373, 0.3322085 ],\n",
       "       [0.33560488, 0.3322394 , 0.3321558 ],\n",
       "       [0.33563635, 0.33227953, 0.3320841 ],\n",
       "       [0.3357312 , 0.33208084, 0.33218798],\n",
       "       [0.33576494, 0.3319748 , 0.33226025],\n",
       "       [0.33570945, 0.33131683, 0.33297375],\n",
       "       [0.33585533, 0.33162785, 0.3325168 ],\n",
       "       [0.3349388 , 0.33028433, 0.3347769 ],\n",
       "       [0.3359374 , 0.33162063, 0.33244196],\n",
       "       [0.33591673, 0.33129966, 0.3327836 ],\n",
       "       [0.33598697, 0.33201542, 0.3319976 ],\n",
       "       [0.33521017, 0.3305815 , 0.3342083 ],\n",
       "       [0.33608487, 0.33175892, 0.3321562 ],\n",
       "       [0.33538845, 0.33071372, 0.33389783],\n",
       "       [0.33530605, 0.33039272, 0.3343012 ],\n",
       "       [0.33528206, 0.33057156, 0.3341464 ],\n",
       "       [0.3343846 , 0.33068955, 0.33492586],\n",
       "       [0.3348202 , 0.33009082, 0.3350889 ],\n",
       "       [0.3344238 , 0.33059078, 0.3349854 ],\n",
       "       [0.3354693 , 0.3298885 , 0.3346422 ],\n",
       "       [0.33429465, 0.33091605, 0.3347893 ],\n",
       "       [0.33464417, 0.3301891 , 0.33516672],\n",
       "       [0.33428216, 0.33094755, 0.3347703 ],\n",
       "       [0.33390018, 0.33190876, 0.33419105],\n",
       "       [0.33529562, 0.32951963, 0.33518472],\n",
       "       [0.33481157, 0.32991773, 0.33527073],\n",
       "       [0.33350107, 0.332912  , 0.3335869 ],\n",
       "       [0.3356911 , 0.329643  , 0.33466592],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33361107, 0.33263555, 0.33375332],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.3341981 , 0.33115914, 0.33464274],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33564708, 0.33151352, 0.33283943],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.333377  , 0.33329895, 0.33332407],\n",
       "       [0.33333945, 0.33332852, 0.33333203],\n",
       "       [0.33334816, 0.33332166, 0.3333302 ],\n",
       "       [0.33334887, 0.3333211 , 0.33333004],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33335942, 0.3333128 , 0.33332783],\n",
       "       [0.33367518, 0.33308786, 0.33323696],\n",
       "       [0.3337001 , 0.33304965, 0.33325025],\n",
       "       [0.33376765, 0.3330733 , 0.33315903],\n",
       "       [0.3335152 , 0.33319005, 0.33329478],\n",
       "       [0.33342916, 0.33325785, 0.33331302],\n",
       "       [0.3337629 , 0.33299497, 0.33324215],\n",
       "       [0.33390972, 0.3329324 , 0.3331579 ],\n",
       "       [0.33396548, 0.33297536, 0.3330591 ],\n",
       "       [0.33399037, 0.3330057 , 0.33300394],\n",
       "       [0.3340396 , 0.33296606, 0.3329943 ],\n",
       "       [0.3340365 , 0.332707  , 0.3332565 ],\n",
       "       [0.33412197, 0.33290341, 0.3329746 ],\n",
       "       [0.33415082, 0.33279026, 0.33305895],\n",
       "       [0.3341197 , 0.33271408, 0.3331662 ],\n",
       "       [0.33424187, 0.33282477, 0.33293337],\n",
       "       [0.33427423, 0.33266446, 0.33306128],\n",
       "       [0.33429295, 0.3328923 , 0.33281472],\n",
       "       [0.33414567, 0.33238384, 0.33347046],\n",
       "       [0.33407512, 0.33221942, 0.33370543],\n",
       "       [0.3344192 , 0.3326005 , 0.33298028],\n",
       "       [0.33411938, 0.33218828, 0.33369234],\n",
       "       [0.33451653, 0.3326893 , 0.3327942 ],\n",
       "       [0.33416772, 0.33213344, 0.3336988 ],\n",
       "       [0.33419162, 0.33182845, 0.33397987],\n",
       "       [0.3342287 , 0.3320267 , 0.33374465],\n",
       "       [0.3345902 , 0.33221042, 0.33319935],\n",
       "       [0.3342907 , 0.33203697, 0.33367226],\n",
       "       [0.33433038, 0.33179262, 0.333877  ],\n",
       "       [0.33435887, 0.33166167, 0.33397943],\n",
       "       [0.33437607, 0.33157605, 0.33404788],\n",
       "       [0.33436725, 0.331494  , 0.3341388 ],\n",
       "       [0.3342713 , 0.33148262, 0.33424607],\n",
       "       [0.33447096, 0.3315539 , 0.33397514],\n",
       "       [0.3343339 , 0.33138055, 0.33428556],\n",
       "       [0.33351526, 0.33287638, 0.33360836],\n",
       "       [0.3342907 , 0.331353  , 0.33435628],\n",
       "       [0.33425456, 0.33135924, 0.33438623],\n",
       "       [0.33343992, 0.33306566, 0.33349442],\n",
       "       [0.33463028, 0.3313594 , 0.3340104 ],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.3340247 , 0.33159557, 0.33437973],\n",
       "       [0.33351052, 0.33288828, 0.33360118],\n",
       "       [0.3334348 , 0.33307853, 0.33348668],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33341405, 0.3331306 , 0.33345535],\n",
       "       [0.3335457 , 0.33279988, 0.33365443],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33430836, 0.3310527 , 0.33463898],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33346894, 0.33299267, 0.33353835],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.3347039 , 0.33225456, 0.33304158],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.3339365 , 0.3328583 , 0.33320522],\n",
       "       [0.33436775, 0.3325189 , 0.33311334],\n",
       "       [0.33488375, 0.33211318, 0.3330031 ],\n",
       "       [0.33446616, 0.33244148, 0.33309233],\n",
       "       [0.33523187, 0.33183956, 0.33292854],\n",
       "       [0.33537167, 0.33172977, 0.33289856],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33541894, 0.33169264, 0.3328884 ],\n",
       "       [0.33603248, 0.33127448, 0.33269304],\n",
       "       [0.3360819 , 0.33125502, 0.33266306],\n",
       "       [0.33568022, 0.3314875 , 0.3328323 ],\n",
       "       [0.33550292, 0.33162668, 0.33287036],\n",
       "       [0.33542314, 0.33168936, 0.3328875 ],\n",
       "       [0.33627465, 0.3311787 , 0.33254665],\n",
       "       [0.33640483, 0.3317536 , 0.33184156],\n",
       "       [0.33525276, 0.33182317, 0.33292407],\n",
       "       [0.33642358, 0.3311325 , 0.33244395],\n",
       "       [0.3363122 , 0.3309916 , 0.33269623],\n",
       "       [0.33660644, 0.33112836, 0.33226526],\n",
       "       [0.33664787, 0.3315524 , 0.33179978],\n",
       "       [0.33661225, 0.33180314, 0.3315846 ],\n",
       "       [0.33669505, 0.33152843, 0.33177656],\n",
       "       [0.3366266 , 0.33056572, 0.33280772],\n",
       "       [0.33680782, 0.33119273, 0.33199948],\n",
       "       [0.33624855, 0.33104154, 0.33270994],\n",
       "       [0.3361798 , 0.33006045, 0.33375978],\n",
       "       [0.33610263, 0.32999796, 0.3338994 ],\n",
       "       [0.33658555, 0.33015242, 0.33326203],\n",
       "       [0.33697602, 0.330689  , 0.33233503],\n",
       "       [0.33589447, 0.32956594, 0.3345396 ],\n",
       "       [0.33596227, 0.32928112, 0.33475658],\n",
       "       [0.33612886, 0.3297954 , 0.33407572],\n",
       "       [0.33598536, 0.32940155, 0.3346131 ],\n",
       "       [0.33625948, 0.3297319 , 0.3340086 ],\n",
       "       [0.3360778 , 0.32889616, 0.3350261 ]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "\n",
    "activation2.output   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333334, 0.33333334, 0.33333334], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation2.output[8]  # batch 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333334, 0.33333334, 0.33333334], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation2.output[4]   # batch 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now to calculate what's right and whats wrong, we will gonna require a loss function.\n",
    "\n",
    "## That's topic for next notebook code ! . . ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
