{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are *Activation Functions* ?\n",
    "\n",
    "## Each neuron in different layers such as hidden as well as output layer of a Neural Network has activation function, and it comes into play right after the *input * weights + bias* output is yeild, and being the output value is being fed to the activation function.\n",
    "\n",
    "## The output from this *Activation Function* then becomes the input for next neurons\n",
    "\n",
    "# *Step Functions* :-\n",
    "\n",
    "### For an activation functions, we can use variety of different mathematical functions. Here we will look at a *Step Function* to start. \n",
    "### The basic idea of a step function is, if the input of a step function is greater than 0, then output will be in 1, otherwise output will be in 0.\n",
    "\n",
    "## y (output) = {1 when x > 0 , 0  when x <=0} , notice the output is between 0 and 1\n",
    "\n",
    "#### So we can consider this *Step Function* to use as an *Activation Function* for our neurons\n",
    "\n",
    "#### But in real use cases, step fuction doesn't seems to solve much of problems at higher levels\n",
    "#### So here we get introduced to another activation function - The Sigmoid\n",
    "\n",
    "# *Sigmoid Activation Funciton* :-\n",
    "\n",
    "### Using *Sigmoid* as activation function seemed to be more reliable and easy while training a neural network due to better granularity of the output\n",
    "### There are other entities such as loss functions, optimizers, etc that we have to deal alongisde the neural networks.\n",
    "\n",
    "### *Optimizers' role is to optimize the neural network by tuning the weights and biases for better results and low loss values, for this they need to know what is the impact of the weights and biases on the outputs. The more granular the output is, the lower the loss*\n",
    "\n",
    "## y(output) = 1 / 1 + e^-x\n",
    "\n",
    "\n",
    "##### With step functions, we can't know how close we are to the output btween 1 and 0\n",
    "\n",
    "# *Rectifying Linear Unit (RELU)* Activation Funciton :-\n",
    "\n",
    "\n",
    "## y(output) = { x when x < 0, 0 when x <=0}, \n",
    "#### which also produces granular output\n",
    "\n",
    "## Now why might we use RELU over Sigmoid ? \n",
    "\n",
    "#### Since, one of the internal reasons is Sigmoid activation function has an issue related to vanishing gradient, (discussed in upcoming jupyter notebooks) \n",
    "## But Two major reason why we use RELU over Sigmoid :-\n",
    "### 1. Its super fast and simple to calculate (straight forward output) than Sigmoid, \n",
    "### 2. It works !\n",
    "\n",
    "## Relu is the most popular Activation function to use in hidden layers of neural networks\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "# But why use Activation function at all ?\n",
    "\n",
    "## If we just use, weights and biases, then we can consider our activation function will be y(output) = x (linear activation function)\n",
    "\n",
    "## With Linear Activation functions, which we get automatically with only using weights and biases, we can only fit linear functions. \n",
    "## If we attempt to fit non linear function, such as *sine wave*, the Neural Network simply can't do or find best fit line.\n",
    "\n",
    "\n",
    "## Hence, we use activation functions to solve this probelm, which gives much better best fit line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refer the Sentdex's Video for awesome visuals for better understanding !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "# Well enough of theoritical concepts, Lets Code !!!\n",
    "\n",
    "\n",
    "X = [[11.8,-2.6,3.2, -5.0],\n",
    "       [1.2,4.8,5.6,-6.1],\n",
    "       [2.0, 5.0, -1.5, 0.6]]   \n",
    "\n",
    "inputs = [0, 2 ,-1 , 3.3,  -2.7, 2.2, -100]\n",
    "\n",
    "output = []\n",
    "\n",
    "for i in inputs:\n",
    "    if i > 0:\n",
    "        output.append(i)\n",
    "    elif i <= 0:\n",
    "        output.append(0)\n",
    "# and this is the algorithm for rectified linear unit activation function        \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 2.2, 0, 0, 2, 0, 3.3, 0, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "## Easier code for the same\n",
    "\n",
    "inputs = [0, 2 ,-1 , 3.3,  -2.7, 2.2, -100]\n",
    "output = []\n",
    "\n",
    "for i in inputs:\n",
    "    output.append(max(0,i))\n",
    "    \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Relu Function to our Class layer_dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now our Classes are ready to rumble, but next thing,\n",
    "## Lets use NNFS package by Sentdex\n",
    "\n",
    "# pip install nnfs\n",
    "\n",
    "### as time goes on, --upgrade will help to use this tool with newer versions\n",
    "\n",
    "## for exact same codes that sentdex's playlist uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "class layer_dense:\n",
    "    def __init__(self,n_inputs, n_neurons):\n",
    "        self.n_neurons = n_neurons\n",
    "        self.weights = 0.10*np.random.randn(n_inputs, n_neurons) \n",
    "        print(\"Created a hidden layer with \"+ str(self.n_neurons)+\" neurons\")\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "    \n",
    "    def forward(self,inputs):   # this method will just take the inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "class activation_relu:\n",
    "        def forward(self, inputs):\n",
    "            self.output= np.maximum(0,inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnfs\n",
    "import numpy as np\n",
    "nnfs.init()  # override and use same datatype , default = float64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to generate our random dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(points,classes):\n",
    "    X = np.zeros(points*classes, 2)\n",
    "    y = np.zeros(points, classes, dtype='unit8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(points*class_number, points*(class_number+1))\n",
    "        r = np.linspace(0,0,1,points)\n",
    "        t = np.linspace(class_number*4, (class_number+1)*4,points+ np.random.randn(points)*0.2)\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5),r*np.cos(t*2.5)]\n",
    "        y[ix] = class_number\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rather using functino we can use nnfs library\n",
    "\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "# spiral data function produces both dataset and labesl\n",
    "\n",
    "X, y = spiral_data(100,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a hidden layer with 5 neurons\n"
     ]
    }
   ],
   "source": [
    "# again recalling, layer_dense(n_inputs, n_neurons), n_inputs = no of features per sample \n",
    "layer1 = layer_dense(2,5)   # here we have two unique features X.shape = 300, 2\n",
    "\n",
    "activation1 = activation_relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00],\n",
       "       [-2.1677322e-03,  2.3993661e-04,  7.8377874e-05,  1.2562921e-03,\n",
       "         2.0973508e-03],\n",
       "       [-5.5519058e-03,  9.1313740e-04,  8.2216633e-04,  1.4294604e-03,\n",
       "         5.2104047e-03],\n",
       "       ...,\n",
       "       [ 2.0824155e-01, -6.1054867e-02, -8.6618371e-02,  1.0688682e-01,\n",
       "        -1.8095909e-01],\n",
       "       [ 2.7096185e-01, -6.4672917e-02, -8.1968546e-02,  5.0633047e-02,\n",
       "        -2.4343792e-01],\n",
       "       [ 1.9220580e-01, -6.0096879e-02, -8.7738618e-02,  1.2107192e-01,\n",
       "        -1.6500287e-01]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1.forward(X)\n",
    "layer1.output\n",
    "\n",
    "# notice lots of negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00],\n",
       "       [0.0000000e+00, 2.3993661e-04, 7.8377874e-05, 1.2562921e-03,\n",
       "        2.0973508e-03],\n",
       "       [0.0000000e+00, 9.1313740e-04, 8.2216633e-04, 1.4294604e-03,\n",
       "        5.2104047e-03],\n",
       "       ...,\n",
       "       [2.0824155e-01, 0.0000000e+00, 0.0000000e+00, 1.0688682e-01,\n",
       "        0.0000000e+00],\n",
       "       [2.7096185e-01, 0.0000000e+00, 0.0000000e+00, 5.0633047e-02,\n",
       "        0.0000000e+00],\n",
       "       [1.9220580e-01, 0.0000000e+00, 0.0000000e+00, 1.2107192e-01,\n",
       "        0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output of layer 1 fed to our relu activation function\n",
    "\n",
    "activation1.forward(layer1.output)\n",
    "\n",
    "activation1.output\n",
    "\n",
    "# notice negative values turned to zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, here, if we can see the neuron's ultimate output tends to have lots of zeros, we can just make bias more bigger in size while defining class, to diagnose the errors "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
