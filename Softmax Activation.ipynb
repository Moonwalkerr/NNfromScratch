{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook explains the Softmax Activation Function in detail\n",
    "\n",
    "\n",
    "\n",
    "## Now first question after learning multiple diff activation functions like Sigmoid and ReLU, why another activation function ?\n",
    "\n",
    "\n",
    "### Imagine these are our output values from a layer :-\n",
    "```sh\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "```\n",
    "### Now what do we do with these ?\n",
    "\n",
    "### If we are only predicting, then we would consider , whichever is the largest value from the layer_outputs is our Prediction. Here layer_outputs[0] = 4.8, is our prediction\n",
    "\n",
    "\n",
    "## We also have to remeber we are trying to train our Neural Networks as well along with the predicting values.\n",
    "\n",
    "### We ll learn in upcoming notebooks on training NNs\n",
    "\n",
    "### But First Step in training the model is to determine *how wrong is our model ?* \n",
    "\n",
    "## Which one of these is more correct ?\n",
    "```sh\n",
    "layer_outputs = [4.8, 1.21, 2.385] \n",
    "(Most People will consider this one as this has relatively larger neuron output at index 0) \n",
    "layer_outputs = [4.8, 4.79, 4.25]\n",
    "```\n",
    "### Accuracy Wise, they are both identical (coz both have 4.8 as prediction)\n",
    "\n",
    "### First step to measure how wrong the output is to compare them relatively to other neurons. This can't be done using ReLU as ReLU doesn't provides per neuron basis comparisions of outputs\n",
    "\n",
    "### Next problem is these both are unbouned, so relatively closeness can vary btw both samples, and there r lots of issues with no solid way to determine how wrong our model is.\n",
    "\n",
    "## This is why we need some new activation function :- *Softmax Activation Function*\n",
    "\n",
    "### Goal :- Let's say we need to classify cats & dogs, we have 2 neurons at output layer. So our goal would be to get the outputs from the 2 neurons in the way of *probability distribution* that if the input is a cat, first neuron shows 0.80 - 0.90 or 1.0 (upto 100 % probability distribution)  and second neuron outputs 0.08, 0.12, etc like values (i.e low confidence score for it being a dog)\n",
    "\n",
    "\n",
    "\n",
    "### This will help us to determine how wrong / right our model is.\n",
    "\n",
    "\n",
    "### If we use a probability distribution, and keep ReLU as function:\n",
    "```sh\n",
    "ReLU(0) = 0,.0000 % \n",
    "ReLU(1) = 1, 100.0000 % \n",
    "```\n",
    "### The problem here is, if any of those ouput values is negative, then ReLU will clip it and yeild Zero.\n",
    "\n",
    "### ReLU(-1) / ReLU(-9000) / ReLU(-21) = 0, 0.000 %\n",
    "\n",
    "#### What if all values are negative ? This doesn't makes any sense\n",
    "#### There are some other methods but research shows they also aren't effective\n",
    "\n",
    "## Exponential Function \n",
    "```sh\n",
    "y = e^x (\"e ~ 2.718281828459045, Euler's Number\") \n",
    "if x = -10.0000\n",
    "y = e^-10.0000 ~ + 0.00005\n",
    "```\n",
    "\n",
    "### What this function actually does is just to make sure no value is actually negative, at the output after passing through it. So we can calculate how accurate the output is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121.51041751873483, 3.353484652549023, 10.859062664920513]\n"
     ]
    }
   ],
   "source": [
    "# Coding for exponentiation function\n",
    "\n",
    "import math\n",
    "\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "\n",
    "E = math.e\n",
    "\n",
    "exp_values = []\n",
    "\n",
    "for output in layer_outputs:\n",
    "    exp_values.append(E**output)\n",
    "print(exp_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After exponentiation, next step is to normalize these values\n",
    "\n",
    "#### In our case, the output of single neuron. is divided by the sum of the output neurons in the output layer\n",
    "\n",
    "#### This gives us the probability distribution that we want.\n",
    "\n",
    "#### This requires us to get rid of all the negative values without losing the information of those negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8952826639572619, 0.024708306782099374, 0.0800090292606387] , Sum of Norm Values :0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "## coding Normalization\n",
    "\n",
    "norm_base = sum(exp_values)\n",
    "norm_values = []\n",
    "\n",
    "for value in exp_values:\n",
    "    norm_values.append(value / norm_base)\n",
    "print(norm_values,\", Sum of Norm Values :\"+str(sum(norm_values)))\n",
    "\n",
    "### exp_values = exponentiated values\n",
    "### norm_values = normalized exponentiated values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quicker way to calculate both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp Values [121.51041752   3.35348465  10.85906266]\n",
      "Norm Exp Values [0.89528266 0.02470831 0.08000903]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs  # not necessary, just following nnfs code in video\n",
    "nnfs.init() # --\"--\n",
    "\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "exp_values = np.exp(layer_outputs)  # numpy's exp function \n",
    "norm_values = exp_values / np.sum(exp_values)\n",
    "print(\"Exp Values\",exp_values)\n",
    "print(\"Norm Exp Values\",norm_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we did here is :-\n",
    "## layer_output -->  Exponentiate --> Normalize --> Output \n",
    "\n",
    "### This can be written as :-\n",
    "## layer_output --> *Softmax* --> Output\n",
    "\n",
    "### Exponentiation and Normalization together are called The Softmax Function ! ! \n",
    "### Now we can easily understand what softmax activation function really is and why we are applying it.\n",
    "\n",
    "# Moving ahead,\n",
    "## In above code, we were dealing with *Single Vector of Output*, but in real, we will be dealing with a *Batch of Outputs*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dynamic code that can deal with batches \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
